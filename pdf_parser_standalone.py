#!/usr/bin/env python3
"""
PDF Parser Standalone - ExtraÃ§Ã£o de dados de resÃ­duos de PDFs CADRI
"""

import sys
from pathlib import Path
import fitz  # pymupdf
import pandas as pd
import re
from typing import List, Dict, Optional, Tuple
from datetime import datetime

# Add src to path for imports
sys.path.append(str(Path(__file__).parent / "src"))

from store_csv import CSVStore
from logging_conf import logger
from config import PDF_DIR, CSV_CADRI_ITEMS, CSV_CADRI_DOCS


class PDFParserStandalone:
    """Parser de PDFs CADRI para extraÃ§Ã£o de dados de resÃ­duos"""

    def __init__(self):
        self.pdf_dir = PDF_DIR
        self.patterns = self._compile_patterns()
        self.parsed_cache = self._load_parsed_cache()
        self.stats = {
            'processed': 0,
            'items_extracted': 0,
            'errors': 0,
            'no_items': 0,
            'skipped': 0
        }

    def _compile_patterns(self) -> Dict[str, re.Pattern]:
        """Compilar regex patterns para extraÃ§Ã£o"""
        return {
            # PadrÃ£o para nÃºmero do resÃ­duo (D099, F001, K001, etc.)
            'numero_residuo': re.compile(
                r'\b([A-Z]\d{3})\b',
                re.IGNORECASE
            ),

            # PadrÃ£o para cÃ³digo de resÃ­duo (formato: XX.XX.XXX) - manter para compatibilidade
            'codigo': re.compile(
                r'(\d{2}[\s\.]?\d{2}[\s\.]?\d{3})',
                re.IGNORECASE
            ),

            # PadrÃ£o para resÃ­duo completo com descriÃ§Ã£o
            'residuo_linha': re.compile(
                r'(\d{1,2})\s+ResÃ­duo\s*:\s*([A-Z]\d{3})\s*-\s*(.+?)(?=\n[A-Z]|\n\n|Classe\s*:|$)',
                re.MULTILINE | re.IGNORECASE | re.DOTALL
            ),

            # PadrÃ£o para classe com estado fÃ­sico e OII
            'classe_detalhada': re.compile(
                r'Classe\s*:\s*([IVX]+[AB]?)\s+Estado\s+FÃ­sico\s*:\s*(\w+)\s+O/?I\s*:\s*([I/O]+)\s+Qtde\s*:\s*([0-9.,]+)\s*([a-zA-Z/\s]+?)(?=\n|ComposiÃ§Ã£o|$)',
                re.IGNORECASE | re.DOTALL
            ),

            # PadrÃ£o para composiÃ§Ã£o aproximada
            'composicao': re.compile(
                r'ComposiÃ§Ã£o\s+Aproximada\s*:\s*(.+?)(?=\s*MÃ©todo\s+Utilizado|$)',
                re.IGNORECASE | re.DOTALL
            ),

            # PadrÃ£o para mÃ©todo utilizado
            'metodo': re.compile(
                r'MÃ©todo\s+Utilizado\s*:\s*(.+?)(?=\s*Cor[,\.]?\s*Cheiro|$)',
                re.IGNORECASE | re.DOTALL
            ),

            # PadrÃ£o para cor, cheiro, aspecto
            'cor_cheiro_aspecto': re.compile(
                r'Cor[,\.]?\s*Cheiro[,\.]?\s*Aspecto\s*:\s*(.+?)(?=\s*Acondicionamento|$)',
                re.IGNORECASE | re.DOTALL
            ),

            # PadrÃ£o para acondicionamento (mÃºltiplos)
            'acondicionamento': re.compile(
                r'Acondicionamento\s*:.*?(?=Destino|$)',
                re.IGNORECASE | re.DOTALL
            ),

            # PadrÃ£o para itens de acondicionamento individuais
            'acondicionamento_item': re.compile(
                r'(?:Acondicionamento\s*:\s*)?(E\d{2})\s*-\s*([^A]+?)(?=\s*Acondicionamento|Destino|$)',
                re.IGNORECASE
            ),

            # PadrÃ£o para destino
            'destino': re.compile(
                r'Destino\s*:\s*(T\d{2})\s*-\s*(.+?)(?=\n\s*\d+\s+ResÃ­duo|$)',
                re.IGNORECASE | re.DOTALL
            ),

            # PadrÃµes melhorados para quantidade e unidade separados
            'quantidade_unidade': re.compile(
                r'Qtde\s*:\s*(\d+[.,]?\d*)\s+([a-zA-Z]+/?[a-zA-Z]*)',
                re.IGNORECASE
            ),

            # PadrÃ£o para quantidade genÃ©rica (fallback)
            'quantidade': re.compile(
                r'(\d+[.,]?\d*)\s*(kg|ton|t|m3|mÂ³|litro|l|unidade|un|peÃ§a|pÃ§|ano)',
                re.IGNORECASE
            ),

            # PadrÃ£o para classe de resÃ­duo genÃ©rica (fallback)
            'classe': re.compile(
                r'classe\s*:\s*(I{1,3}[AB]?|I{1,2}\s*[AB])',
                re.IGNORECASE
            ),

            # PadrÃ£o para estado fÃ­sico
            'estado_fisico': re.compile(
                r'Estado\s+FÃ­sico\s*:\s*(\w+)',
                re.IGNORECASE
            ),

            # PadrÃ£o para OII
            'oii': re.compile(
                r'OII\s*:\s*([^\s]+)',
                re.IGNORECASE
            ),

            # Identificar tipo de documento
            'doc_type': re.compile(
                r'(CADRI|Certificado.*MovimentaÃ§Ã£o|LicenÃ§a|CERT.*MOV.*RESIDUOS)',
                re.IGNORECASE
            ),

            # Data de validade
            'validade': re.compile(
                r'(validade|vÃ¡lida?\s+atÃ©|vencimento).*?(\d{2}[/-]\d{2}[/-]\d{4})',
                re.IGNORECASE
            ),

            # NÃºmero do documento
            'numero_doc': re.compile(
                r'n[ÃºÂºÂ°]?\s*(\d{5,})',
                re.IGNORECASE
            ),

            # PadrÃµes de limpeza de texto
            'clean_spaces': re.compile(r'\s+'),
            'clean_breaks': re.compile(r'[\n\r]+'),
        }

    def _load_parsed_cache(self) -> set:
        """Carregar lista de PDFs jÃ¡ processados"""
        try:
            if Path(CSV_CADRI_ITEMS).exists():
                df = pd.read_csv(CSV_CADRI_ITEMS)
                return set(df['numero_documento'].unique())
        except Exception as e:
            logger.debug(f"Erro ao carregar cache: {e}")
        return set()

    def parse_pdf(self, pdf_path: Path) -> List[Dict]:
        """
        Extrair dados de resÃ­duos de um PDF

        Args:
            pdf_path: Caminho do arquivo PDF

        Returns:
            Lista de itens extraÃ­dos
        """
        items = []
        numero_documento = pdf_path.stem

        try:
            # Abrir PDF
            with fitz.open(pdf_path) as doc:
                # Extrair texto de todas as pÃ¡ginas
                full_text = ""
                for page in doc:
                    full_text += page.get_text()

                # Limpeza inicial do texto
                full_text = self._clean_text(full_text)

                # Identificar tipo de documento
                doc_type = self._identify_doc_type(full_text)
                if not doc_type:
                    logger.warning(f"Tipo de documento nÃ£o identificado: {numero_documento}")
                    doc_type = "DOCUMENTO"

                # Extrair data de validade
                validade = self._extract_date(full_text, 'validade')

                # Tentar extraÃ§Ã£o com nova lÃ³gica estruturada primeiro
                items = self._extract_residuos_enhanced(
                    full_text,
                    numero_documento,
                    doc_type,
                    validade
                )

                logger.debug(f"Enhanced parsing found {len(items)} items for {numero_documento}")

                # Se nÃ£o encontrou nada, tentar mÃ©todo antigo como fallback
                if not items:
                    logger.debug(f"Enhanced parsing failed, trying structured approach for {numero_documento}")
                    residuos_section = self._find_residuos_section(full_text)
                    if residuos_section:
                        items = self._extract_residuos_structured(
                            residuos_section,
                            numero_documento,
                            doc_type,
                            validade
                        )
                    else:
                        logger.debug(f"Structured approach failed, trying alternative for {numero_documento}")
                        items = self._extract_residuos_alternative(
                            full_text,
                            numero_documento,
                            doc_type,
                            validade
                        )

                # Validar e limpar itens extraÃ­dos
                items = self.validate_extraction(items)

                logger.info(f"ğŸ“„ {numero_documento}: {len(items)} itens extraÃ­dos")

        except Exception as e:
            logger.error(f"Erro ao processar {pdf_path}: {e}")
            self.stats['errors'] += 1

        return items

    def _clean_text(self, text: str) -> str:
        """Limpar e normalizar texto"""
        # Normalizar espaÃ§os
        text = self.patterns['clean_spaces'].sub(' ', text)
        # Normalizar quebras de linha
        text = self.patterns['clean_breaks'].sub('\n', text)
        return text.strip()

    def _identify_doc_type(self, text: str) -> Optional[str]:
        """Identificar tipo de documento"""
        match = self.patterns['doc_type'].search(text)
        if match:
            doc_type = match.group(1).upper()
            # Normalizar
            if 'CADRI' in doc_type:
                return 'CADRI'
            elif 'CERTIFICADO' in doc_type or 'CERT' in doc_type:
                return 'CERT_MOV_RESIDUOS'
            elif 'LICENÃ‡A' in doc_type:
                return 'LICENCA'
        return None

    def _extract_date(self, text: str, date_type: str) -> Optional[str]:
        """Extrair data do texto"""
        if date_type == 'validade':
            match = self.patterns['validade'].search(text)
            if match:
                date_str = match.group(2)
                # Normalizar formato
                date_str = date_str.replace('/', '-')
                try:
                    date_obj = datetime.strptime(date_str, '%d-%m-%Y')
                    return date_obj.strftime('%Y-%m-%d')
                except:
                    return date_str
        return None

    def _find_residuos_section(self, text: str) -> Optional[str]:
        """Localizar seÃ§Ã£o de resÃ­duos no texto"""
        # Marcadores de inÃ­cio da seÃ§Ã£o de resÃ­duos
        markers = [
            r'RESÃDUOS\s+AUTORIZADOS',
            r'RELAÃ‡ÃƒO\s+DE\s+RESÃDUOS',
            r'RESÃDUOS\s+CLASSE',
            r'CÃ“DIGO.*DESCRIÃ‡ÃƒO.*QUANTIDADE',
            r'ITEM.*RESÃDUO.*CLASSE',
            r'RESÃDUOS\s+A\s+SEREM\s+TRATADOS',
            r'LISTA\s+DE\s+RESÃDUOS'
        ]

        for marker in markers:
            pattern = re.compile(marker, re.IGNORECASE | re.DOTALL)
            match = pattern.search(text)
            if match:
                # Retornar texto a partir do marcador
                start_pos = match.start()
                # Pegar prÃ³ximos 3000 caracteres
                section = text[start_pos:start_pos+3000]
                logger.debug(f"SeÃ§Ã£o encontrada com marcador: {marker}")
                return section

        return None

    def _extract_residuos_structured(
        self,
        section: str,
        numero_documento: str,
        doc_type: str,
        validade: Optional[str]
    ) -> List[Dict]:
        """Extrair resÃ­duos de uma seÃ§Ã£o estruturada"""
        items = []
        lines = section.split('\n')
        current_item = {}

        for line_num, line in enumerate(lines):
            line = line.strip()

            if not line or len(line) < 5:
                # Linha vazia ou muito curta, salvar item se completo
                if current_item.get('codigo') and current_item.get('descricao'):
                    items.append(self._create_item_dict(
                        current_item, numero_documento, doc_type, validade
                    ))
                    current_item = {}
                continue

            # Procurar cÃ³digo de resÃ­duo
            codigo_match = self.patterns['codigo'].search(line)
            if codigo_match:
                # Salvar item anterior se existir
                if current_item.get('codigo') and current_item.get('descricao'):
                    items.append(self._create_item_dict(
                        current_item, numero_documento, doc_type, validade
                    ))

                # Iniciar novo item
                current_item = {
                    'codigo': self._normalize_codigo(codigo_match.group(1)),
                    'descricao': line[codigo_match.end():].strip(),
                    'linha_original': line
                }

                # Procurar dados adicionais na mesma linha
                self._extract_additional_data(line, current_item)
                continue

            # Se jÃ¡ temos um cÃ³digo, adicionar Ã  descriÃ§Ã£o
            if current_item.get('codigo'):
                if current_item.get('descricao'):
                    current_item['descricao'] += ' ' + line
                else:
                    current_item['descricao'] = line

                # Procurar dados adicionais
                self._extract_additional_data(line, current_item)

        # Salvar Ãºltimo item se existir
        if current_item.get('codigo') and current_item.get('descricao'):
            items.append(self._create_item_dict(
                current_item, numero_documento, doc_type, validade
            ))

        return items

    def _extract_residuos_alternative(
        self,
        text: str,
        numero_documento: str,
        doc_type: str,
        validade: Optional[str]
    ) -> List[Dict]:
        """ExtraÃ§Ã£o alternativa quando nÃ£o hÃ¡ seÃ§Ã£o clara"""
        items = []

        # Procurar todos os cÃ³digos de resÃ­duo
        codigos = self.patterns['codigo'].findall(text)
        codigos_unicos = list(set(codigos))

        for codigo in codigos_unicos:
            codigo_norm = self._normalize_codigo(codigo)

            # Tentar encontrar contexto ao redor do cÃ³digo
            escaped_codigo = re.escape(codigo)
            pattern = re.compile(
                rf'{escaped_codigo}[^\n]*(?:\n[^\n]*)?',
                re.IGNORECASE
            )
            match = pattern.search(text)

            if match:
                context = match.group(0)

                # Extrair descriÃ§Ã£o (texto apÃ³s o cÃ³digo)
                codigo_pos = context.find(codigo)
                if codigo_pos >= 0:
                    descricao = context[codigo_pos + len(codigo):].strip()
                    descricao = re.sub(r'\s+', ' ', descricao)[:500]

                    item_data = {
                        'codigo': codigo_norm,
                        'descricao': descricao,
                        'linha_original': context.replace('\n', ' ')
                    }

                    # Extrair dados adicionais
                    self._extract_additional_data(context, item_data)

                    items.append(self._create_item_dict(
                        item_data, numero_documento, doc_type, validade
                    ))

        return items

    def _extract_additional_data(self, text: str, item_data: Dict):
        """Extrair quantidade, unidade e classe do texto"""
        # Procurar quantidade
        qtd_match = self.patterns['quantidade'].search(text)
        if qtd_match and not item_data.get('quantidade'):
            item_data['quantidade'] = qtd_match.group(1).replace(',', '.')
            item_data['unidade'] = qtd_match.group(2).lower()

        # Procurar classe
        classe_match = self.patterns['classe'].search(text)
        if classe_match and not item_data.get('classe'):
            item_data['classe'] = classe_match.group(1).replace(' ', '')

    def _normalize_codigo(self, codigo: str) -> str:
        """Normalizar cÃ³digo de resÃ­duo para formato XX.XX.XXX"""
        # Remover espaÃ§os e caracteres especiais
        codigo_clean = re.sub(r'[^\d]', '', codigo)

        # Adicionar pontos no formato correto
        if len(codigo_clean) == 7:
            return f"{codigo_clean[:2]}.{codigo_clean[2:4]}.{codigo_clean[4:]}"

        return codigo

    def _create_item_dict(
        self,
        item_data: Dict,
        numero_documento: str,
        doc_type: str,
        validade: Optional[str]
    ) -> Dict:
        """Criar dicionÃ¡rio padronizado do item"""
        return {
            'numero_documento': numero_documento,
            'tipo_documento': doc_type,
            'codigo_residuo': item_data.get('codigo', ''),
            'descricao_residuo': item_data.get('descricao', ''),
            'quantidade': item_data.get('quantidade', ''),
            'unidade': item_data.get('unidade', ''),
            'classe': item_data.get('classe', ''),
            'data_validade': validade,
            'observacao': '',
            'linha_original': item_data.get('linha_original', ''),
            'data_extracao': datetime.now().isoformat()
        }

    def _extract_residuos_enhanced(
        self,
        text: str,
        numero_documento: str,
        doc_type: str,
        validade: Optional[str]
    ) -> List[Dict]:
        """
        ExtraÃ§Ã£o melhorada baseada na estrutura identificada na imagem

        Procura por blocos estruturados contendo:
        - NÃºmero do resÃ­duo (ex: D099)
        - DescriÃ§Ã£o completa
        - Classe, Estado FÃ­sico, OII, Quantidade
        - ComposiÃ§Ã£o, MÃ©todo, Cor/Cheiro/Aspecto
        - Acondicionamento e Destino
        """
        items = []

        # Procurar por padrÃµes de inÃ­cio de resÃ­duo: "01 ResÃ­duo : D099 - ..."
        residuo_matches = self.patterns['residuo_linha'].finditer(text)

        for match in residuo_matches:
            item_numero = match.group(1)
            numero_residuo = match.group(2)
            descricao_inicial = match.group(3)

            # Encontrar o bloco completo do resÃ­duo
            start_pos = match.start()

            # Procurar prÃ³ximo resÃ­duo ou fim do texto
            next_match = None
            for next_candidate in self.patterns['residuo_linha'].finditer(text[start_pos + len(match.group(0)):]):
                next_match = next_candidate
                break

            if next_match:
                end_pos = start_pos + len(match.group(0)) + next_match.start()
            else:
                # Se nÃ£o hÃ¡ prÃ³ximo, pegar prÃ³ximos 2000 caracteres
                end_pos = min(start_pos + 2000, len(text))

            # Extrair bloco completo do resÃ­duo
            residuo_block = text[start_pos:end_pos]

            # Extrair campos estruturados do bloco
            item_data = self._extract_structured_fields(residuo_block)

            # Dados bÃ¡sicos
            item_data.update({
                'item_numero': item_numero.zfill(2),
                'numero_residuo': numero_residuo,
                'descricao_residuo': descricao_inicial.strip(),
                'raw_fragment': residuo_block[:500]  # Primeiros 500 chars
            })

            # Criar item completo
            complete_item = self._create_enhanced_item_dict(
                item_data, numero_documento, doc_type, validade
            )

            if complete_item:
                items.append(complete_item)
                logger.debug(f"ExtraÃ­do resÃ­duo {numero_residuo}: {descricao_inicial[:50]}...")

        return items

    def _extract_structured_fields(self, block: str) -> Dict[str, str]:
        """Extrair campos estruturados de um bloco de resÃ­duo"""
        fields = {}

        # Classe, Estado FÃ­sico, OII, Quantidade (em uma linha)
        classe_match = self.patterns['classe_detalhada'].search(block)
        if classe_match:
            fields['classe_residuo'] = classe_match.group(1)
            fields['estado_fisico'] = classe_match.group(2).upper()
            fields['oii'] = classe_match.group(3)
            fields['quantidade'] = classe_match.group(4).replace(',', '.')
            # Normalizar unidade (remover " / ano" etc.)
            unidade_raw = classe_match.group(5).strip()
            fields['unidade'] = unidade_raw.split()[0] if unidade_raw else ''
        else:
            # Tentar padrÃµes individuais

            # Quantidade e unidade
            qtd_match = self.patterns['quantidade_unidade'].search(block)
            if qtd_match:
                fields['quantidade'] = qtd_match.group(1).replace(',', '.')
                fields['unidade'] = qtd_match.group(2)

            # Classe
            classe_match = self.patterns['classe'].search(block)
            if classe_match:
                fields['classe_residuo'] = classe_match.group(1)

            # Estado fÃ­sico
            estado_match = self.patterns['estado_fisico'].search(block)
            if estado_match:
                fields['estado_fisico'] = estado_match.group(1).upper()

            # OII
            oii_match = self.patterns['oii'].search(block)
            if oii_match:
                fields['oii'] = oii_match.group(1)

        # ComposiÃ§Ã£o aproximada
        comp_match = self.patterns['composicao'].search(block)
        if comp_match:
            fields['composicao_aproximada'] = comp_match.group(1).strip()

        # MÃ©todo utilizado
        metodo_match = self.patterns['metodo'].search(block)
        if metodo_match:
            fields['metodo_utilizado'] = metodo_match.group(1).strip()

        # Cor, cheiro, aspecto
        cor_match = self.patterns['cor_cheiro_aspecto'].search(block)
        if cor_match:
            fields['cor_cheiro_aspecto'] = cor_match.group(1).strip()

        # Acondicionamento - extrair todos os cÃ³digos E## do bloco
        e_codes = re.findall(r'E\d{2}', block)
        if e_codes:
            # Remover duplicatas mantendo ordem
            unique_codes = []
            for code in e_codes:
                if code not in unique_codes:
                    unique_codes.append(code)

            fields['acondicionamento_codigos'] = ','.join(unique_codes)

            # Tentar extrair descriÃ§Ãµes correspondentes
            descricoes = []
            for code in unique_codes:
                # Procurar padrÃ£o "E## - descriÃ§Ã£o"
                desc_match = re.search(rf'{code}\s*-\s*([^A-Z]+?)(?=\s+[A-Z]|$)', block, re.IGNORECASE)
                if desc_match:
                    desc = desc_match.group(1).strip()
                    # Limpar descriÃ§Ã£o
                    desc = desc.split('Acondicionamento')[0].strip()
                    descricoes.append(desc)
                else:
                    descricoes.append('')

            fields['acondicionamento_descricoes'] = ' | '.join(filter(None, descricoes))

        # Destino
        destino_match = self.patterns['destino'].search(block)
        if destino_match:
            fields['destino_codigo'] = destino_match.group(1)
            fields['destino_descricao'] = destino_match.group(2).strip()

        return fields

    def _create_enhanced_item_dict(
        self,
        item_data: Dict,
        numero_documento: str,
        doc_type: str,
        validade: Optional[str]
    ) -> Optional[Dict]:
        """Criar dicionÃ¡rio de item com novos campos expandidos"""

        # ValidaÃ§Ã£o bÃ¡sica
        if not item_data.get('numero_residuo') or not item_data.get('descricao_residuo'):
            return None

        return {
            'numero_documento': numero_documento,
            'item_numero': item_data.get('item_numero', '01'),
            'numero_residuo': item_data.get('numero_residuo', ''),
            'descricao_residuo': item_data.get('descricao_residuo', ''),
            'classe_residuo': item_data.get('classe_residuo', ''),
            'estado_fisico': item_data.get('estado_fisico', ''),
            'oii': item_data.get('oii', ''),
            'quantidade': item_data.get('quantidade', ''),
            'unidade': item_data.get('unidade', ''),
            'composicao_aproximada': item_data.get('composicao_aproximada', ''),
            'metodo_utilizado': item_data.get('metodo_utilizado', ''),
            'cor_cheiro_aspecto': item_data.get('cor_cheiro_aspecto', ''),
            'acondicionamento_codigos': item_data.get('acondicionamento_codigos', ''),
            'acondicionamento_descricoes': item_data.get('acondicionamento_descricoes', ''),
            'destino_codigo': item_data.get('destino_codigo', ''),
            'destino_descricao': item_data.get('destino_descricao', ''),
            'pagina_origem': 1,  # TODO: detectar pÃ¡gina correta
            'raw_fragment': item_data.get('raw_fragment', ''),
            'tipo_documento': doc_type,
            'data_validade': validade,
            'updated_at': datetime.now().isoformat()
        }

    def validate_extraction(self, items: List[Dict]) -> List[Dict]:
        """Validar e limpar dados extraÃ­dos"""
        validated = []

        for item in items:
            # Validar se Ã© novo schema (numero_residuo) ou antigo (codigo_residuo)
            numero_residuo = item.get('numero_residuo', '')
            codigo_residuo = item.get('codigo_residuo', '')

            # Para novo schema: validar formato [A-Z]\d{3} (D099, F001, etc.)
            if numero_residuo:
                if not re.match(r'^[A-Z]\d{3}$', numero_residuo):
                    logger.debug(f"NÃºmero de resÃ­duo invÃ¡lido ignorado: {numero_residuo}")
                    continue
            # Para schema antigo: validar formato XX.XX.XXX
            elif codigo_residuo:
                if not re.match(r'^\d{2}\.\d{2}\.\d{3}$', codigo_residuo):
                    logger.debug(f"CÃ³digo de resÃ­duo invÃ¡lido ignorado: {codigo_residuo}")
                    continue
            else:
                logger.debug("Item sem cÃ³digo ou nÃºmero de resÃ­duo vÃ¡lido")
                continue

            # Limpar descriÃ§Ã£o
            descricao = item.get('descricao_residuo', '')
            if descricao:
                descricao = re.sub(r'\s+', ' ', descricao).strip()[:500]
                item['descricao_residuo'] = descricao

            # Normalizar unidade
            unidade = item.get('unidade', '').lower()
            unidade_map = {
                'kg': 'kg',
                'ton': 't',
                't': 't',
                'm3': 'mÂ³',
                'mÂ³': 'mÂ³',
                'l': 'L',
                'litro': 'L',
                'litros': 'L',
                'unidade': 'un',
                'unidades': 'un',
                'un': 'un',
                'peÃ§a': 'un',
                'peÃ§as': 'un',
                'pÃ§': 'un'
            }
            item['unidade'] = unidade_map.get(unidade, unidade)

            # Normalizar quantidade
            quantidade = item.get('quantidade', '')
            if quantidade:
                try:
                    # Converter para float e de volta para string para normalizar
                    quantidade_float = float(quantidade)
                    item['quantidade'] = str(quantidade_float)
                except:
                    pass

            validated.append(item)

        return validated

    def parse_all_pdfs(self, filter_type: str = None, force_reparse: bool = False) -> Dict[str, int]:
        """
        Processar todos os PDFs pendentes

        Args:
            filter_type: Filtrar por tipo de documento (opcional)

        Returns:
            EstatÃ­sticas do processamento
        """
        # Listar PDFs
        pdf_files = list(self.pdf_dir.glob("*.pdf"))

        # Filtrar jÃ¡ processados (a menos que force_reparse seja True)
        if force_reparse:
            pending = pdf_files
            logger.info("Modo forÃ§a reprocessamento ativado - processando todos os PDFs")
        else:
            pending = [
                pdf for pdf in pdf_files
                if pdf.stem not in self.parsed_cache
            ]

        if not pending:
            logger.info("Nenhum PDF novo para processar")
            return self.stats

        # Filtrar por tipo se especificado
        if filter_type:
            # Carregar info dos documentos
            try:
                df_docs = pd.read_csv(CSV_CADRI_DOCS)
                # Filtrar PDFs por tipo
                docs_filtered = df_docs[df_docs['tipo_documento'] == filter_type]
                docs_numbers = set(docs_filtered['numero_documento'].astype(str))
                pending = [
                    pdf for pdf in pending
                    if pdf.stem in docs_numbers
                ]
                logger.info(f"Filtrado por tipo '{filter_type}': {len(pending)} PDFs")
            except Exception as e:
                logger.warning(f"Erro ao filtrar por tipo: {e}")

        logger.info(f"ğŸ“„ {len(pending)} PDFs para processar")

        all_items = []

        for i, pdf_path in enumerate(pending, 1):
            logger.info(f"[{i}/{len(pending)}] Processando: {pdf_path.name}")

            try:
                items = self.parse_pdf(pdf_path)

                if items:
                    all_items.extend(items)
                    self.stats['items_extracted'] += len(items)
                else:
                    self.stats['no_items'] += 1

                self.stats['processed'] += 1

            except Exception as e:
                logger.error(f"Erro em {pdf_path.name}: {e}")
                self.stats['errors'] += 1

        # Salvar no CSV
        if all_items:
            self._save_items_to_csv(all_items)

        return self.stats

    def _save_items_to_csv(self, items: List[Dict]):
        """Salvar itens extraÃ­dos no CSV com novo schema expandido"""
        try:
            from store_csv import CSVStore, CSVSchemas

            # Garantir que o CSV existe com o schema correto
            CSVStore.ensure_csv(Path(CSV_CADRI_ITEMS), CSVSchemas.CADRI_ITEMS_COLS)

            df_new = pd.DataFrame(items)

            # Se o DataFrame nÃ£o estÃ¡ vazio, usar upsert do CSVStore
            if not df_new.empty:
                # Usar nÃºmero do documento + nÃºmero do resÃ­duo como chave Ãºnica
                keys = ['numero_documento', 'numero_residuo']

                # Se numero_residuo nÃ£o existe, usar campos antigos para compatibilidade
                if 'numero_residuo' not in df_new.columns and 'codigo_residuo' in df_new.columns:
                    keys = ['numero_documento', 'codigo_residuo']
                elif 'item_numero' in df_new.columns:
                    # Para novo schema expandido, usar documento + item_numero como chave
                    keys = ['numero_documento', 'item_numero']

                CSVStore.upsert(df_new, Path(CSV_CADRI_ITEMS), keys)
                logger.info(f"âœ… Salvos {len(items)} itens em {CSV_CADRI_ITEMS}")
            else:
                logger.warning("Nenhum item para salvar")

        except Exception as e:
            logger.error(f"Erro ao salvar items: {e}")
            # Fallback para mÃ©todo antigo em caso de erro
            try:
                df_new = pd.DataFrame(items)
                df_new.to_csv(CSV_CADRI_ITEMS, index=False)
                logger.info(f"âœ… Salvos {len(items)} itens em {CSV_CADRI_ITEMS} (fallback)")
            except Exception as e2:
                logger.error(f"Erro no fallback: {e2}")


def main():
    """FunÃ§Ã£o principal"""
    import argparse

    parser = argparse.ArgumentParser(description='PDF Parser Standalone')
    parser.add_argument(
        '--type',
        type=str,
        help='Filtrar por tipo de documento (ex: CERT MOV RESIDUOS INT AMB)'
    )
    parser.add_argument(
        '--file',
        type=str,
        help='Processar apenas um arquivo especÃ­fico'
    )
    parser.add_argument(
        '--document',
        type=str,
        help='Processar apenas um documento especÃ­fico pelo nÃºmero'
    )
    parser.add_argument(
        '--force-reparse',
        action='store_true',
        help='ForÃ§ar reprocessamento mesmo se jÃ¡ foi parseado'
    )

    args = parser.parse_args()

    logger.info("=" * 70)
    logger.info("PDF Parser Standalone - ExtraÃ§Ã£o Aprimorada de Dados de ResÃ­duos")
    if args.type:
        logger.info(f"Tipo: {args.type}")
    if args.file:
        logger.info(f"Arquivo: {args.file}")
    if args.document:
        logger.info(f"Documento: {args.document}")
    if args.force_reparse:
        logger.info("Modo: ForÃ§ar reprocessamento")
    logger.info("=" * 70)

    parser_instance = PDFParserStandalone()

    if args.file:
        # Processar arquivo especÃ­fico
        pdf_path = Path(args.file)
        if not pdf_path.exists():
            pdf_path = parser_instance.pdf_dir / args.file
            if not pdf_path.exists():
                logger.error(f"Arquivo nÃ£o encontrado: {args.file}")
                return

        items = parser_instance.parse_pdf(pdf_path)
        if items:
            parser_instance._save_items_to_csv(items)

        stats = {
            'processed': 1,
            'items_extracted': len(items),
            'errors': 0 if items else 1,
            'no_items': 1 if not items else 0
        }
    elif args.document:
        # Processar documento especÃ­fico pelo nÃºmero
        pdf_path = parser_instance.pdf_dir / f"{args.document}.pdf"
        if not pdf_path.exists():
            logger.error(f"PDF nÃ£o encontrado para documento: {args.document}")
            return

        items = parser_instance.parse_pdf(pdf_path)
        if items:
            parser_instance._save_items_to_csv(items)

        stats = {
            'processed': 1,
            'items_extracted': len(items),
            'errors': 0 if items else 1,
            'no_items': 1 if not items else 0
        }
    else:
        # Processar todos os PDFs
        stats = parser_instance.parse_all_pdfs(args.type, args.force_reparse)

    # Mostrar estatÃ­sticas
    logger.info("=" * 70)
    logger.info("ğŸ“Š EstatÃ­sticas finais:")
    logger.info(f"   PDFs processados: {stats['processed']}")
    logger.info(f"   Itens extraÃ­dos: {stats['items_extracted']}")
    logger.info(f"   Sem itens: {stats['no_items']}")
    logger.info(f"   Erros: {stats['errors']}")
    logger.info("=" * 70)

    if stats['processed'] > 0:
        media_itens = stats['items_extracted'] / stats['processed']
        logger.info(f"   ğŸ“ˆ MÃ©dia de itens por PDF: {media_itens:.1f}")

    return stats


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("\nâš ï¸  Processamento interrompido pelo usuÃ¡rio")
    except Exception as e:
        logger.error(f"Erro fatal: {e}")
        sys.exit(1)